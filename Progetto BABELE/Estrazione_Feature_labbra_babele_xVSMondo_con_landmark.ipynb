{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29776,"status":"ok","timestamp":1685444820706,"user":{"displayName":"CARMINE D'ANGELO","userId":"02677852198280047247"},"user_tz":-120},"id":"F-lAM7RD3qSe","outputId":"db886029-d626-405f-f07c-e8f8047d637c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Zuvq7TBq2D5X"},"source":["# **Prime Librerie da importare**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo8NKpg6KD-U"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import dlib\n","import math\n","import os\n","from pathlib import Path"]},{"cell_type":"markdown","source":["# Path principali delle cartelle in cui sono contenuti i file"],"metadata":{"id":"BH_ZfORY6ecK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZTO9djE7BJP"},"outputs":[],"source":["path_principale = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/\"\n","path_train = \"Train/\"\n","path_test = \"Test/\"\n","path_validation = \"Validation/\""]},{"cell_type":"markdown","metadata":{"id":"klRcJ0hshwZQ"},"source":["# **Salvataggio lista di librerie e versioni installate**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6fAXSWKhwpu"},"outputs":[],"source":["!pip freeze > \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/requirements_estrazioneFeatures.txt\""]},{"cell_type":"markdown","metadata":{"id":"bPslwNSTLuOg"},"source":["# **Creazioni directory importanti per il progetto**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqag5w4LK4Te"},"outputs":[],"source":["# Funzione per creare una directory se non esiste gi√†\n","def create_directory_if_not_exists(directory):\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","# Creazione delle directory principali\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI\")\n","create_directory_if_not_exists(path_principale + \"FILE_PATH\")\n","\n","# Creazione delle sotto-directory per train, test e validation\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI/\" + path_test)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train + \"TWO_FRAME/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test + \"TWO_FRAME/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation + \"TWO_FRAME/\")\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train + \"SEQUENCE/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test + \"SEQUENCE/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation + \"SEQUENCE/\")\n","\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_validation)"]},{"cell_type":"markdown","metadata":{"id":"OU7A_9tjKHXD"},"source":["# **Configurazione mediapipe**"]},{"cell_type":"markdown","metadata":{"id":"SH0wRyX1I_uc"},"source":["## Installazione mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIFMaYcI1Yjh"},"outputs":[],"source":["!pip install mediapipe"]},{"cell_type":"markdown","metadata":{"id":"M9N_J_7OKJVL"},"source":["## Coordinate dei landmark delle labbra di mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pB4JnEDH39pv"},"outputs":[],"source":["FACEMESH_LIPS = [(61,61),(37,37),(0, 0)(267,267),(306,306),(314,314),(17, 17),(84,84)]"]},{"cell_type":"markdown","metadata":{"id":"8Hq0X9Y82NmQ"},"source":["# **Funzioni per il calcolo dei landmark**"]},{"cell_type":"markdown","metadata":{"id":"uxQBPpihQY9v"},"source":["## **Funzione che calcola la distanza tra due landmark sfruttando la distanza cityblock**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrIaes7G1u3z"},"outputs":[],"source":["from scipy.spatial import distance\n","\n","def calcola_distanza_landmark(selected_points,  dist=\"cityblock\"):\n","  if any(math.isnan(val) for coord in selected_points for val in coord):\n","        return math.nan\n","  # Inizializzazione della variabile per la lunghezza totale\n","  total_length = 0\n","\n","  # Iterazione su tutti i landmark selezionati in ordine\n","  for i in range(len(selected_points)-1):\n","      # Estrazione delle coordinate del landmark corrente\n","      p1= selected_points[i]\n","      \n","      # Estrazione delle coordinate del landmark successivo\n","      p2 = selected_points[i+1]\n","      # cityblock, cosine, chebyshev, euclidean\n","      # cityblock, cosine, chebyshev, euclidean\n","      if dist == \"cityblock\":\n","        distanza = distance.cityblock(p1, p2)\n","      elif dist == \"cosine\":\n","        distanza = distance.cosine(p1, p2)\n","      elif dist == \"chebyshev\":\n","        distanza = distance.chebyshev(p1, p2)\n","      elif dist == \"euclidean\":\n","        distanza = distance.euclidean(p1, p2)\n","      else:\n","        raise ValueError(\"Valore di dist non riconosciuto\")\n","      \n","      \n","      # Aggiornamento della lunghezza totale\n","      total_length += distanza\n","      #total_length = round(total_length,2)\n","  \n","  return total_length"]},{"cell_type":"markdown","metadata":{"id":"N4ItQLDYKxhZ"},"source":["## **Funzione che estrae la larghezza delle labbra in pixel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3bt9qW21zuf"},"outputs":[],"source":["# larghezza delle labbra su tre punti specifici\n","def estrazione_feature_larghezza(landmarks_points, distanza=\"cityblock\"):\n","\n","  zona_superiore = [landmarks_points[37],  landmarks_points[267]]\n","\n","  zona_centrale =  [landmarks_points[61],  landmarks_points[306] ]\n","\n","  zona_inferiore = [landmarks_points[84],  landmarks_points[314]]\n","\n","  w1 = calcola_distanza_landmark(zona_superiore, distanza)\n","  w2 = calcola_distanza_landmark(zona_centrale, distanza)\n","  w3 = calcola_distanza_landmark(zona_inferiore, distanza)\n","\n","  return {\"37-267\":w1, \"61-306\": w2,\"84-314\": w3}"]},{"cell_type":"markdown","metadata":{"id":"S1g_xhwoK8no"},"source":["## **Funzione che estrae l'altezza delle labbra in pixel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtcrKE6e17K3"},"outputs":[],"source":["# Altezza delle labbra utilizzando i tubercoli\n","def estrazione_feature_altezza(landmarks_points, distanza=\"cityblock\"):\n","\n","  tubercolo_sinistro_superiore = [landmarks_points[37],  landmarks_points[84]]\n","\n","  tubercolo_centrale_superiore = [landmarks_points[0],  landmarks_points[17]]\n","\n","  tubercolo_destro_superiore = [landmarks_points[267],  landmarks_points[314]]\n","\n","  h1 = calcola_distanza_landmark(tubercolo_sinistro_superiore,  distanza)\n","  h2 = calcola_distanza_landmark(tubercolo_centrale_superiore,  distanza)\n","  h3 = calcola_distanza_landmark(tubercolo_destro_superiore, distanza)\n","\n","\n","  return {\"37-84\":h1,\"0-17\": h2,\"267-314\":h3}"]},{"cell_type":"markdown","metadata":{"id":"ZbbaQtajLDIn"},"source":["## **Funzione utilizzata per settare i valori dei landmark a (0,0) in caso che il volto non venga rilevato**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnACv3Va-lGB"},"outputs":[],"source":["# funzione che controlla se un landmark √® stato trovato, se non √® stato trovato setta il suo valore a NaN\n","def key_exists(landmarks_points):\n","  temp_value = [math.nan, math.nan]\n","  if 37 not in landmarks_points:\n","    landmarks_points[37] = temp_value\n","  if 267 not in landmarks_points:\n","    landmarks_points[267] = temp_value\n","  if 61 not in landmarks_points:\n","    landmarks_points[61] = temp_value\n","  if 306 not in landmarks_points:\n","    landmarks_points[306] = temp_value\n","  if 84 not in landmarks_points:\n","    landmarks_points[84] = temp_value\n","  if 314 not in landmarks_points:\n","    landmarks_points[314] = temp_value\n","  if 0 not in landmarks_points:\n","    landmarks_points[0] = temp_value\n","  if 17 not in landmarks_points:\n","    landmarks_points[17] = temp_value"]},{"cell_type":"markdown","metadata":{"id":"Gdyv13G7LMqD"},"source":["## **Funzione che genera un dizionario contenente le feature delle labbra**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCzUqdjKGzVP"},"outputs":[],"source":["def generaFeature(landmarks_points,distanza=\"cityblock\"):\n","  key_exists(landmarks_points)\n","  feature2 = estrazione_feature_larghezza(landmarks_points, distanza)\n","  feature3 = estrazione_feature_altezza(landmarks_points, distanza)\n","\n","  return dict(feature2,  **feature3)"]},{"cell_type":"markdown","metadata":{"id":"7gRCp5XBqE4C"},"source":["# **Funzione per toglierre i NaN**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7cPHouJ6-Ia"},"outputs":[],"source":["import pandas as pd\n","\n","\n","# Funzione che riempie i valori NaN con la media mobile calcolata su una finestra mobile di lunghezza n_frame\n","# Il tutto a condizione che ci siano almeno 2 valori non NaN nella finestra (min_periods)\n","def fill_nan(row, n_frame):\n","    return row.fillna(row.rolling(min_periods=2, window=n_frame).mean())\n","\n","# Funzione per sostituire i valori NaN all'inizio del file\n","def remove_initial_nan(df):\n","  # Sostituzione dei valori NaN nelle colonne intermedie con la media dei successivi\n","  df_filled = df.copy()\n","  columns_to_fill = df.columns[1:-1]  # Seleziona tutte le colonne tranne la prima e l'ultima\n","\n","  for column in columns_to_fill:\n","      df_filled[column].fillna(df_filled[column].fillna(method='ffill').shift(-1).mean(), inplace=True)\n","    \n","  return df_filled\n","\n","# Funzione per rimuovere i valori NaN\n","def remove_nan(df,num_frames_per_second):\n","  df = remove_initial_nan(df)\n","  df = df.apply(fill_nan, axis=1, n_frame=num_frames_per_second) #Caso di NaN in altre posizioni\n","\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"h0DGakXtLYkw"},"source":["# **Generazione dei file in cui saranno contenute le path dei video**"]},{"cell_type":"markdown","metadata":{"id":"lAOdKkSK-nNm"},"source":["## Creazione file path video Babele"]},{"cell_type":"markdown","metadata":{"id":"ANVMo5pqzKrV"},"source":["La nomenclatura proposta √® stata:\n","*Language_gender_age_id-unique_framerate*\n","\n","In merito a questo il csv in output sar√† cos√¨ formato:\n","*   Nome del file: *Language_gender_age_id-unique_framerate_numeroframe*\n","*   Features estratte: distanza labbra , ecc\n","*   Target: lingua associata al frame con le feature estratte\n","\n","\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-Dt_f3R-yCT"},"outputs":[],"source":["import os\n","import glob\n","\n","# cartella contenente la path del dataset\n","dataset_path = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Datasets/Dataset_2_BABELE/BABELE_Dataset/Dataset_split_subject_independent/Full_video_10_seconds_division_train_test_val\"\n","\n","# cartella in cui salvare i file\n","folder_save = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH\"\n","\n","# Loop over all subfolders in folder_read\n","for subfolder in os.listdir(dataset_path):\n","    # Create a file with the subfolder's name in folder_save\n","    file_path = os.path.join(folder_save, subfolder + \"_babele.txt\")\n","    \n","    # Open the file for writing\n","    with open(file_path, \"w\") as f:\n","        # Get a list of all files in the subfolder\n","        files = glob.glob(os.path.join(dataset_path, subfolder, \"*\"))\n","        \n","        # Write each file's path to the file\n","        for file in files:\n","            f.write(file + \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"GH7bRntSLggB"},"source":["# **Preprocessing dataset lingue vs Mondo**\n","\n","*   Estrazione dei landmark dai frame video\n","*   Calcolo distanze fra i landmark\n","*   Creazione di un file csv per ogni file di testo che contiene le path dei video\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8V2qWDnqJ7mB"},"source":["## Lista contenente le path dei file dei video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbG8JeD33yJd"},"outputs":[],"source":["files_path = [\"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Test_babele.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Validation_babele.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Train_babele.txt\"\n","              ]"]},{"cell_type":"markdown","metadata":{"id":"dd4tM905i88J"},"source":["## **Creazione dataset binario Tedesco vs Mondo**"]},{"cell_type":"markdown","metadata":{"id":"FCdLgBNJFqLz"},"source":["### Bilanciamento dataset tra Tedesco e altre lingue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hwh1kC6FpSh"},"outputs":[],"source":["# Funzione che divide il dataset in due sottoinsiemi in base alla classe di appartenenza\n","def bilanciamento_dataset(df):\n","  class_0 = df[df['target'] == 0]\n","  class_1 = df[df['target'] == 1]\n","\n","  # determiniamo qunati elementi prendere dalla classe di maggioranza basandoci sul numero di elementi disponibili\n","  # della classe di minoranza\n","  num_samples = len(class_1)\n","\n","  # seleziona un numero casuale di campioni dalla classe di maggioranza\n","  # random_state √® usato come seed di partenza per la randomizzazione\n","  class_0_balanced = class_0.sample(n=num_samples, random_state=42)\n","\n","  # unisce i due sottoinsiemi bilanciati\n","  balanced_dataset = pd.concat([class_0_balanced, class_1])\n","\n","  # mischia il dataset in modo casuale\n","  balanced_dataset = balanced_dataset.sample(frac=1, random_state=42)\n","\n","  # stampa il conteggio delle classi nel dataset bilanciato\n","  print(balanced_dataset['target'].value_counts())\n","\n","  return balanced_dataset"]},{"cell_type":"markdown","metadata":{"id":"xl_reT8G4x5R"},"source":["### Creazione dataset tra Tedesco e altre lingue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taV76tPFjHUI"},"outputs":[],"source":["import random\n","import cv2\n","import mediapipe as mp\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","fps = 24\n","num_frames = 240\n","num_frames_per_second = 6\n","num_seconds = num_frames // fps\n","\n","random_indices = []\n","for i in range(num_seconds):\n","    start_index = i * fps\n","    end_index = start_index + fps\n","    indices = list(range(start_index, end_index))\n","    random_indices.extend(random.sample(indices, num_frames_per_second))\n","\n","random_indices.sort()\n","\n","frames_to_process = [i for i in range(num_frames) if i in random_indices]\n","\n","print(frames_to_process)\n","\n","# Caricamento del modello di rilevamento dei landmarks del volto\n","mp_drawing = mp.solutions.drawing_utils\n","mp_face_mesh = mp.solutions.face_mesh\n","\n","# Inizializza il rilevatore dei landmark del viso\n","face_mesh = mp_face_mesh.FaceMesh()\n","distanza=\"cityblock\"\n","\n","for path_file in files_path:\n","\n","  with open(path_file, mode='r') as file:\n","\n","    video_data = []  # Lista di tutti i dati dei video\n","    \n","    linee = file.readlines()\n","\n","    csv_filename = os.path.basename(path_file)\n","    csv_filename_without_extension = csv_filename.split(\".\")[0] #per la creazione del dataset voglio un nome senza .txt\n","    estensione = \"_cityblock_tedescoVsMondo.csv\"\n","    \n","    if \"Validation\" in path_file:\n","      csv_path = path_principale+\"CSV_DATASET/\"+path_validation + csv_filename_without_extension + estensione\n","    elif \"Train\" in path_file:\n","      csv_path = path_principale+\"CSV_DATASET/\"+path_train + csv_filename_without_extension + estensione\n","    elif \"Test\" in path_file:\n","       csv_path = path_principale+\"CSV_DATASET/\"+path_test + csv_filename_without_extension + estensione\n","    else:\n","      print(\"Qualcosa √® andato storto!\")\n","      break\n","\n","    for video_path in tqdm(linee, desc=\"Elaborazione video \" + csv_filename_without_extension, unit=\"video\"):\n","        video_path = video_path.strip() # Rimuovi il carattere di a capo dalla stringa\n","\n","        # Apre il video in input\n","        cap = cv2.VideoCapture(str(video_path))\n","\n","        filename = os.path.basename(video_path) #Nome del file video\n","        filename_without_extension = filename.split(\".\")[0] #per la creazione del dataset voglio un nome senza .avi\n","        components = filename.split(\"_\")[:-1] #splitto l'array in un altro per \"_\" e elimino \".avi\"\n","       \n","        target = int(components[0]) #Estrazione classe (lingua)\n","\n","        if target == 3:\n","            target = 1\n","        else:\n","            target = 0\n","\n","        # Inizializza la lista dei dati del video\n","        video_data_video = []\n","\n","        # Loop attraverso tutti i frame del video\n","        frame_count = 0\n","        frame_data = {}\n","        #nome del frame\n","        frame_data['video-frame'] = filename_without_extension # Alla fine collego al nome del file il frame analizzato\n","        colonna = 0\n","\n","       \n","        while cap.isOpened() and frame_count < 240:\n","            \n","\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            if frame_count not in frames_to_process: #l'indice del frame non si trova in quelli random\n","              frame_count += 1\n","              continue\n","          \n","            # Converte l'immagine in RGB\n","            frame_RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            \n","            if frame_RGB is None:\n","              print(\"Frame errato, la funzione cv2.COLOR_BGR2RGB non √® stata in grado di convertire l'immagine in RGB\")\n","              frame_count += 1\n","              continue\n","            else:\n","              results = face_mesh.process(frame_RGB)\n","\n","              if results is None:\n","                print(\"Qualcosa √® andato storto!\")\n","                frame_count += 1\n","                continue\n","              else:\n","                # Memorizza i punti dei landmarks delle labbra\n","                landmarks_points = {}\n","                if results.multi_face_landmarks:\n","                    face_landmarks = results.multi_face_landmarks[0]\n","                    for connection in FACEMESH_LIPS:\n","                        landmark_1 = face_landmarks.landmark[connection[0]]\n","                        n = connection[0]\n","\n","                        # Salva i dati del landmark\n","                        landmarks_points[n] = [landmark_1.x, landmark_1.y]\n","\n","                \n","                feature = generaFeature(landmarks_points, distanza)\n","\n","                video_data_video\n","                \n","                # Crea il dizionario con i dati del frame\n","                frame_data['37-267_'+str(colonna)] = feature['37-267']\n","                frame_data['61-306_'+str(colonna)]= feature['61-306']\n","                frame_data['84-314_'+str(colonna)]= feature['84-314']\n","                frame_data['37-84_'+str(colonna)]= feature['37-84']\n","                frame_data['0-17_'+str(colonna)]= feature['0-17']\n","                frame_data['267-314_'+str(colonna)]= feature['267-314']\n","\n","                frame_count += 1\n","                colonna += 1 \n","\n","        frame_data['target']=  target\n","        \n","        # Aggiungi il dizionario alla lista dei dati del video\n","        video_data_video.append(frame_data)\n","\n","        # Aggiungi i dati del video alla lista di tutti i dati\n","        video_data.extend(video_data_video)\n","\n","        # Chiudi il video\n","        cap.release()\n","\n","    # Crea il dataframe dai dati di tutti i video e visualizzalo\n","    df = pd.DataFrame(video_data)\n","\n","    ## Check per valori NaN\n","    df = remove_nan(df,(num_frames_per_second*10))\n","\n","    dataset_balanced = bilanciamento_dataset(df) \n","    # Visualizza il DataFrame\n","    display(dataset_balanced , num_frames_per_second)\n","\n","    print(100*\"-\")\n","    print(\"Generazione del file csv \", csv_filename_without_extension, estensione)\n","    print(100*\"-\")\n","    dataset_balanced.to_csv(csv_path, index=False)\n","\n","    file.close()\n","\n","# Chiudi il rilevatore dei landmark del viso\n","face_mesh.close()"]},{"cell_type":"markdown","metadata":{"id":"hvU1M_Tv48ey"},"source":["## Creazione dataset binarie tra le lingue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKX47pfz9QdZ"},"outputs":[],"source":["def genera_dataset_binario(df,folder,file_name,lan_target):\n","  # Crea una copia del dataset\n","  df_copia = df.copy()\n","\n","  # Estrai il valore di \"Language\" dal video-frame\n","  df_copia[\"Language\"] = df_copia[\"video-frame\"].str.split(\"_\").str[0]\n","\n","  # Modifica la colonna \"target\" in base al valore della colonna \"language\"\n","  df[\"target\"] = df_copia[\"Language\"].apply(lambda x: 1 if x == lan_target else 0)\n","\n","  #bilanciamento del dataset\n","  dataset_balanced = bilanciamento_dataset(df) \n","\n","  # Salva la copia del dataset modificato su un nuovo file CSV\n","  dataset_balanced.to_csv(path_principale+\"CSV_DATASET/\"+folder+file_name+\".csv\", index=False)  # Assumi che tu voglia salvare il nuovo file come \"nome_file_modificato.csv\"\n","\n","  print(\"Dataset \" + file_name+\".csv creato correttamente\" )"]},{"cell_type":"markdown","source":["## Generazione effettiva dei dataset"],"metadata":{"id":"izoYDaAucBM6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"phE99lO99TmS"},"outputs":[],"source":["import pandas as pd\n","\n","#Recupero dei vari csv\n","df_train = pd.read_csv(path_principale+'CSV_DATASET/'+path_train+'Train_babele_cityblock_60FPS.csv')\n","df_test = pd.read_csv(path_principale+'CSV_DATASET/'+path_test+'Test_babele_cityblock_60FPS.csv')\n","df_validation = pd.read_csv(path_principale+'CSV_DATASET/'+path_validation+'Validation_babele_cityblock_60FPS.csv')\n","\n","\n","lingue = {\n","    1: \"italiano\",\n","    2: \"inglese\",\n","    3: \"tedesco\",\n","    4: \"spagnolo\",\n","    5: \"olandese\",\n","    6: \"russo\",\n","    7: \"giapponese\",\n","    8: \"francese\"\n","}\n","\n","for key in lingue:\n","  if key !=3: # dataset per il tedesco gi√† creato\n","    genera_dataset_binario(df_train, path_train, \"Train_babele_cityblock_\"+lingue[key]+\"VsMondo\",str(key))\n","    genera_dataset_binario(df_test, path_test, \"Test_babele_cityblock_\"+lingue[key]+\"VsMondo\",str(key))\n","    genera_dataset_binario(df_validation, path_validation, \"Validation_babele_cityblock_\"+lingue[key]+\"VsMondo\",str(key))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}