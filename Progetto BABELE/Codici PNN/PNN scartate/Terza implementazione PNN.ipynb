{"cells":[{"cell_type":"markdown","metadata":{"id":"amXO7GRIh6LI"},"source":["# Terza probabile implementazione di PNN ☣\n","https://github.com/antoniogrv/probabilistic-neural-network - *link di riferimento*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22830,"status":"ok","timestamp":1683790439263,"user":{"displayName":"Francesco Aurilio","userId":"13284204835978863805"},"user_tz":-120},"id":"O2Qul3SICFxI"},"outputs":[{"ename":"MessageError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-d5df0069828e\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--\u003e 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZEYPjTED6H7e"},"source":["# **Import librerie**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-JxWtiWu6HN1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score  # importa alcune funzioni di valutazione delle prestazioni del modello\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale"]},{"cell_type":"markdown","metadata":{"id":"6Ob7zfC1iNPF"},"source":["# Caricamento dataset iris con suddivisione in training set e test set\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpklHU03ieG4"},"outputs":[],"source":["# Definisce una funzione denominata \"input()\" che non riceve argomenti in ingresso\n","def input():\n","    # Carica i dati del dataset Iris utilizzando il metodo \"load_iris()\" del modulo \"datasets\" di scikit-learn\n","    iris = datasets.load_iris()\n","    # Assegna le caratteristiche ai dati contenuti nella variabile \"x\"\n","    x = iris.data\n","    # Assegna le etichette ai dati contenuti nella variabile \"y\"\n","    y = iris.target\n","    \n","    # Scala le caratteristiche del dataset \"x\" utilizzando la funzione \"scale()\" del modulo \"preprocessing\" di scikit-learn\n","    x = scale(x)\n","    \n","    # Utilizza la funzione \"train_test_split()\" del modulo \"model_selection\" di scikit-learn per dividere il dataset in un set di training e un set di test\n","    # Imposta una proporzione del 70% di dati per il training set e del 30% di dati per il test set\n","    # Imposta un valore di \"random_state\" a 0 per garantire la riproducibilità dei risultati\n","    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n","    \n","    # Crea un dizionario \"data\" contenente i quattro array di dati risultanti dalla suddivisione del dataset\n","    data = {'x_train': x_train, \n","            'x_test': x_test, \n","            'y_train': y_train, \n","            'y_test': y_test}\n","    \n","    \n","    # Restituisce il dizionario \"data\" come risultato della funzione \"input()\"\n","    return data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CxjtboytOkuL"},"source":["# Caricamento datasets BABELE\n","\n","\n","*   Training set\n","*   Validation set\n","*   Test set\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IngTAYY0P5kI"},"source":["## Caricamento Training set BABELE\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVmkyigLPV08"},"outputs":[],"source":["df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Biometria/CSV_DATASET/Train_babele_cityblock.csv')\n","\n","# Rimuove le colonna \"target\" e \"video-frame\" dal dataset e assegna i dati rimanenti alla matrice delle features X\n","X_train = df_train.drop(columns=['target', 'video-frame'], axis=1); #TRAIN X\n","print(50*\"-\"+\"X_TRAIN\"+\"-\"*50)\n","print(X_train)\n","\n","# Seleziona la colonna \"target\" e assegna i dati all'array delle etichette Y\n","Y_train = df_train[['target']] #TRAIN Y\n","print(50*\"-\"+\"Y_TRAIN\"+\"-\"*50)\n","print(Y_train)"]},{"cell_type":"markdown","metadata":{"id":"zT5fC-tTPz14"},"source":["## Caricamento Validation set BABELE\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1683392911858,"user":{"displayName":"Matteo Della Rocca","userId":"11927604185525197706"},"user_tz":-120},"id":"ExZjj85sC8ql","outputId":"ce682981-d3e2-45ff-8556-dfd67db194d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------X_VALIDATION--------------------------------------------------\n","         37-267     61-306     84-314      37-84       0-17    267-314\n","0      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000\n","1      9.000000  27.018512   8.000000  15.132746  15.132746  15.033296\n","2      8.000000  26.019224   8.000000  11.000000  10.000000  11.000000\n","3      8.000000  25.000000   8.062258  13.000000  12.000000  12.000000\n","4      7.000000  24.000000   8.000000  13.000000  13.000000  13.038405\n","...         ...        ...        ...        ...        ...        ...\n","1595  10.440307  30.805844   9.219544  14.317821  14.560220  13.601471\n","1596  10.198039  32.984845   9.219544  12.369317  13.601471  12.649111\n","1597   9.055385  31.400637  10.198039  15.297059  16.278821  16.124515\n","1598  10.198039  31.575307  10.198039  15.132746  16.124515  15.132746\n","1599  13.038405  42.426407  13.152946  21.213203  22.203603  22.203603\n","\n","[1600 rows x 6 columns]\n","--------------------------------------------------Y_VALIDATION--------------------------------------------------\n","      target\n","0          2\n","1          2\n","2          2\n","3          2\n","4          2\n","...      ...\n","1595       8\n","1596       8\n","1597       8\n","1598       8\n","1599       8\n","\n","[1600 rows x 1 columns]\n"]}],"source":["df_validation = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Biometria/CSV_DATASET/Validation.csv')\n","\n","# Rimuove le colonna \"target\" e \"video-frame\" dal dataset e assegna i dati rimanenti alla matrice delle features X\n","X_validation = df_validation.drop(columns=['target', 'video-frame'], axis=1); #TRAIN X\n","print(50*\"-\"+\"X_VALIDATION\"+\"-\"*50)\n","print(X_validation)\n","\n","# Seleziona la colonna \"target\" e assegna i dati all'array delle etichette Y\n","Y_validation = df_validation[['target']] \n","print(50*\"-\"+\"Y_VALIDATION\"+\"-\"*50)\n","print(Y_validation)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZIBm4CUhP9hE"},"source":["## Caricamento Test set BABELE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1683790566026,"user":{"displayName":"Francesco Aurilio","userId":"13284204835978863805"},"user_tz":-120},"id":"QpM_9AnSPXp3","outputId":"f851983b-3081-4d58-8be1-072e600fc18c"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------X_TEST--------------------------------------------------\n","     37-267_0  61-306_0  84-314_0   37-84_0    0-17_0  267-314_0  37-267_1  \\\n","0    0.027979  0.091484  0.027880  0.047988  0.047445   0.048191  0.021962   \n","1    0.015919  0.053563  0.015976  0.038931  0.038009   0.038874  0.018831   \n","2    0.017207  0.053732  0.019266  0.069050  0.070789   0.071110  0.023425   \n","3    0.019881  0.061997  0.020145  0.047103  0.046868   0.047540  0.018061   \n","4    0.017456  0.057951  0.017618  0.039854  0.039597   0.040044  0.015534   \n","..        ...       ...       ...       ...       ...        ...       ...   \n","155  0.016625  0.057409  0.016617  0.036661  0.036249   0.036652  0.014969   \n","156  0.015001  0.048505  0.015162  0.036214  0.036365   0.036392  0.011436   \n","157  0.015019  0.048462  0.015710  0.046680  0.046549   0.045075  0.014538   \n","158  0.012699  0.040637  0.013195  0.029949  0.029456   0.029610  0.012480   \n","159  0.010600  0.033786  0.012753  0.040110  0.039777   0.039731  0.011140   \n","\n","     61-306_1  84-314_1   37-84_1  ...  84-314_8   37-84_8    0-17_8  \\\n","0    0.072803  0.020721  0.037016  ...  0.021440  0.048025  0.047575   \n","1    0.055941  0.020548  0.069319  ...  0.021332  0.041757  0.041887   \n","2    0.072995  0.024350  0.054368  ...  0.025891  0.059271  0.059383   \n","3    0.053708  0.017818  0.046066  ...  0.016905  0.050505  0.049767   \n","4    0.049762  0.017203  0.057272  ...  0.024030  0.067590  0.069568   \n","..        ...       ...       ...  ...       ...       ...       ...   \n","155  0.049998  0.014306  0.035298  ...  0.012444  0.036728  0.036298   \n","156  0.035212  0.012559  0.040485  ...  0.012576  0.037193  0.036662   \n","157  0.049000  0.015250  0.030487  ...  0.011998  0.027169  0.026677   \n","158  0.040731  0.012638  0.027047  ...  0.013452  0.027954  0.027491   \n","159  0.036338  0.012299  0.035512  ...  0.012382  0.038183  0.039158   \n","\n","     267-314_8  37-267_9  61-306_9  84-314_9   37-84_9    0-17_9  267-314_9  \n","0     0.048264  0.018926  0.056137  0.019407  0.053961  0.054732   0.054727  \n","1     0.041663  0.018265  0.057270  0.018992  0.056680  0.057211   0.057691  \n","2     0.059857  0.016043  0.052432  0.017552  0.049107  0.049453   0.050616  \n","3     0.049196  0.022006  0.073669  0.021600  0.050167  0.049460   0.049612  \n","4     0.069574  0.016245  0.053427  0.016626  0.049931  0.049450   0.049550  \n","..         ...       ...       ...       ...       ...       ...        ...  \n","155   0.035796  0.011739  0.035894  0.013101  0.043823  0.043947   0.042943  \n","156   0.036536  0.012639  0.040663  0.013882  0.041264  0.041228   0.040673  \n","157   0.027268  0.013040  0.041715  0.013709  0.036543  0.036422   0.036313  \n","158   0.027883  0.015356  0.051386  0.015488  0.033365  0.033551   0.033497  \n","159   0.039798  0.012291  0.039614  0.013145  0.037072  0.037957   0.038503  \n","\n","[160 rows x 60 columns]\n","     target\n","0         1\n","1         1\n","2         1\n","3         1\n","4         1\n","..      ...\n","155       8\n","156       8\n","157       8\n","158       8\n","159       8\n","\n","[160 rows x 1 columns]\n"]}],"source":["df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Biometria/CSV_DATASET/Test_babele_cityblock.csv')\n","\n","# Rimuove la colonna \"target\" dal dataset e assegna i dati rimanenti alla matrice delle features X\n","X_test = df_test.drop(columns=['target', 'video-frame'], axis=1); #TRAIN X\n","print(50*\"-\"+\"X_TEST\"+\"-\"*50)\n","print(X_test)\n","\n","# Seleziona la colonna \"target\" e assegna i dati all'array delle etichette Y\n","Y_test = df_test[['target']] #TRAIN Y\n","print(Y_test)"]},{"cell_type":"markdown","metadata":{"id":"Vgnf_L4pkkpe"},"source":["# Implementazione dell'algoritmo di classificazione PNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VdKf-DUviEyz"},"outputs":[],"source":["# Funzione di base radiale utilizzata nella PNN\n","def rbf(centre, x, sigma):  \n","    centre = centre.reshape(1, -1)  # riformatta il centro in un array 1D\n","    temp = -np.sum((centre - x) ** 2, axis=1)  # calcola la distanza quadratica tra il centro e i punti di dati\n","    temp = temp / (2 * sigma * sigma)  # applica il parametro di larghezza di banda\n","    temp = np.exp(temp)  # applica la funzione esponenziale\n","    gaussian = np.sum(temp)  # somma i valori calcolati\n","    return gaussian  # restituisce il valore finale\n","\n","def subset_by_class(data, labels):  # funzione per creare sottoinsiemi di dati per classe\n","    x_train_subsets = []  # lista vuota per i sottoinsiemi di dati\n","    for l in labels:  # per ogni classe di etichette\n","        indices = np.where(data['y_train'] == l)  # seleziona gli indici dei punti di dati che appartengono alla classe corrente\n","        x_train_subsets.append(data['x_train'][indices, :])  # aggiungi i punti di dati nella lista di sottoinsiemi\n","    return x_train_subsets  # restituisci la lista di sottoinsiemi di dati per ciascuna classe\n","\n","def PNN(data, sigma):  # funzione per l'algoritmo di classificazione PNN\n","    num_testset = data['x_test'].shape[0]  # numero di punti di dati nel set di test\n","    labels = np.unique(data['y_train'])  # etichette uniche nel set di training\n","    num_class = len(labels)  # numero di classi\n","    print(\"Numero di classi riscontrato: \", num_class)\n","    sigma = sigma  # parametro di larghezza di banda\n","    x_train_subsets = subset_by_class(data, labels)  # crea sottoinsiemi di dati per ciascuna classe\n","    summation_layer = np.zeros(num_class)  # inizializza il vettore per la somma dei valori di ciascuna classe\n","    predictions = np.zeros(num_testset)  # inizializza il vettore per le etichette di classificazione predette\n","    for i, test_point in enumerate(data['x_test']):  # per ogni punto di dati nel set di test\n","        for j, subset in enumerate(x_train_subsets):  # per ogni sottoinsieme di dati di ogni classe\n","            summation_layer[j] = np.sum(rbf(test_point, subset[0], sigma)) / subset[0].shape[0]  # calcola la somma dei valori di ciascuna classe\n","        predictions[i] = np.argmax(summation_layer)  # assegna all'etichetta predetta la classe corrispondente al valore massimo del vettore di somma\n","    return predictions  # restituisce le etichette di classificazione predette per il set di test\n","\n","def print_metrics(y_test, predictions):  # funzione per stampare le metriche di valutazione delle prestazioni del modello\n","    print('Confusion Matrix')\n","    print(confusion_matrix(y_test, predictions))  # stampa la matrice di confusione\n","    print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))  # stampa l'accuratezza\n","    print('Precision: {}'.format(precision_score(y_test, predictions, average='micro')))  # stampa la precisione\n","    print('Recall: {}'.format(recall_score(y_test, predictions, average='micro')))  # stampa il richiamo\n"]},{"cell_type":"markdown","metadata":{"id":"zbK9Phbjom_5"},"source":["\n","\n","# Predizione su singolo input"]},{"cell_type":"markdown","metadata":{"id":"6w6JlnNCpREW"},"source":["La funzione predict_PNN prende in input un singolo fiore rappresentato da un array random_flower, che viene passato alla funzione PNN per effettuare la classificazione dei dati di input. In particolare, l'array random_flower viene convertito in un array 2D x_test contenente una singola riga, che viene utilizzato come input per la funzione PNN.\n","\n","Quindi, la funzione predict_PNN è progettata per effettuare la predizione della classe di un singolo fiore. Se si desidera effettuare la predizione di più fiori contemporaneamente, è possibile modificare la funzione predict_PNN per accettare un array di fiori come input e chiamare la funzione PNN con l'intero array di dati di input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hapcN390orbp"},"outputs":[],"source":["# Funzione per la predizione dei dati di input\n","def predict_PNN(random_flower, data):\n","    # crea un dizionario contenente i dati di input e le etichette di classe corrispondenti\n","    data = {\n","        'x_test': np.array([random_flower]),  # converte i dati di input in un array 2D\n","        'y_test': np.array([0]),  # assegna un'etichetta di classe fittizia, che verrà sovrascritta dalla funzione PNN\n","        'y_train': data['y_train'],  # array 1D con le etichette di classe del set di training\n","        'x_train': data['x_train']  # array 2D con i valori di input del set di training\n","    }\n","\n","    # chiama la funzione PNN per effettuare la classificazione dei dati di input\n","    predicted_class = PNN(data)\n","\n","    return predicted_class"]},{"cell_type":"markdown","metadata":{"id":"5xLforLHkc8O"},"source":["# Esecuzione su test set di Babele\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1683790576121,"user":{"displayName":"Francesco Aurilio","userId":"13284204835978863805"},"user_tz":-120},"id":"xdyCyOE3kcSv","outputId":"841303e2-b064-4d80-fc25-f880a535ef07"},"outputs":[{"name":"stdout","output_type":"stream","text":["Numero di classi riscontrato:  8\n","[4. 4. 4. 4. 4. 4. 7. 4. 7. 7. 4. 4. 4. 4. 4. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n"," 2. 4. 4. 4. 7. 7. 4. 2. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 7. 7. 7.\n"," 7. 7. 4. 5. 7. 7. 7. 7. 7. 7. 7. 7. 5. 5. 5. 5. 5. 7. 7. 7. 7. 7. 5. 5.\n"," 5. 5. 5. 5. 5. 5. 5. 5. 7. 4. 4. 4. 4. 2. 2. 2. 2. 4. 2. 2. 2. 1. 2. 2.\n"," 1. 1. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 4. 2. 2. 4. 2. 2. 4. 4.\n"," 4. 4. 7. 7. 5. 5. 4. 7. 4. 7. 7. 7. 7. 5. 7. 2. 2. 2. 2. 1. 7. 7. 7. 7.\n"," 7. 4. 4. 4. 4. 2. 7. 7. 7. 7. 7. 5. 5. 5. 5. 5.]\n","Confusion Matrix\n","[[ 5  0  0 12  0  0  3  0]\n"," [ 2 12  0  5  0  0  1  0]\n"," [ 5 15  0  0  0  0  0  0]\n"," [ 0  6  0 12  0  0  2  0]\n"," [ 0  0  0  6  1  0 13  0]\n"," [ 0  0  0  0 15  0  5  0]\n"," [ 0  1  0  8  3  0  8  0]\n"," [ 0  1  0  4  5  0 10  0]]\n","Accuracy: 0.2375\n","Precision: 0.2375\n","Recall: 0.2375\n"]}],"source":["# acquisisce i dati di input, prende i test set\n","#data = input()\n","data = {'x_train': X_train.values, \n","            'x_test': X_test.values, \n","            'y_train': Y_train.values, \n","            'y_test': Y_test.values}\n","\n","sigma = 0.30 #Scelgo il sigma 0.30\n","\n","predictions = PNN(data, sigma)  # chiama la funzione PNN per effettuare la classificazione dei dati di input\n","print(predictions)  # stampa le etichette di classificazione predette per i dati di input\n","print_metrics(data['y_test'], predictions)  "]},{"cell_type":"markdown","metadata":{"id":"1WxcGdRzoOzF"},"source":["# Esecuzione su Random Flower"]},{"cell_type":"markdown","metadata":{"id":"dEOx6rT7pV8A"},"source":["esegue la predizione della classe di un singolo fiore rappresentato dall'array random_flower e capire se dà lo stesso output delle altre considerate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682770034519,"user":{"displayName":"CARMINE D'ANGELO","userId":"02677852198280047247"},"user_tz":-120},"id":"_sMmM7IWmt_n","outputId":"0bf743e5-bf26-49db-c801-e9594396671d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dati del fiore in input:  [3.1 4.5 5.2 2.1]\n","Classe predetta: [2.]\n"]}],"source":["# Esegui la predizione sui dati di input\n","random_flower = np.array([3.1, 4.5, 5.2, 2.1])  # definisce i dati di input\n","predicted_class = predict_PNN(random_flower, data)\n","\n","print(\"Dati del fiore in input: \", random_flower)\n","# stampa la classe predetta\n","print(\"Classe predetta:\", predicted_class)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}