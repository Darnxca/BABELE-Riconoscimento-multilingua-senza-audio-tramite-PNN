{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29776,"status":"ok","timestamp":1685444820706,"user":{"displayName":"CARMINE D'ANGELO","userId":"02677852198280047247"},"user_tz":-120},"id":"F-lAM7RD3qSe","outputId":"db886029-d626-405f-f07c-e8f8047d637c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Zuvq7TBq2D5X"},"source":["# **Prime Librerie da importare**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo8NKpg6KD-U"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import dlib\n","import math\n","import os\n","from pathlib import Path"]},{"cell_type":"markdown","source":["# Path principali delle cartelle in cui sono contenuti i file"],"metadata":{"id":"h6hR1c6r6xF1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZTO9djE7BJP"},"outputs":[],"source":["path_principale = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/\"\n","path_train = \"Train/\"\n","path_test = \"Test/\"\n","path_validation = \"Validation/\""]},{"cell_type":"markdown","metadata":{"id":"klRcJ0hshwZQ"},"source":["# **Salvataggio lista di librerie e versioni installate**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6fAXSWKhwpu"},"outputs":[],"source":["!pip freeze > \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/requirements_estrazioneFeatures.txt\""]},{"cell_type":"markdown","metadata":{"id":"bPslwNSTLuOg"},"source":["# **Creazioni directory importanti per il progetto**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqag5w4LK4Te"},"outputs":[],"source":["# Funzione per creare una directory se non esiste già\n","def create_directory_if_not_exists(directory):\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","# Creazione delle directory principali\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED\")\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI\")\n","create_directory_if_not_exists(path_principale + \"FILE_PATH\")\n","\n","# Creazione delle sotto-directory per train, test e validation\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_MERGED/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_SVD/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_MERGED/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_PNN_RESULT_SVD/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_CH-SIMS_CMU-MOSEI/\" + path_test)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation)\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train + \"TWO_FRAME/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test + \"TWO_FRAME/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation + \"TWO_FRAME/\")\n","\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_train + \"SEQUENCE/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_test + \"SEQUENCE/\")\n","create_directory_if_not_exists(path_principale + \"CSV_DATASET_OPTICAL_FLOW/\" + path_validation + \"SEQUENCE/\")\n","\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_train)\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_test)\n","create_directory_if_not_exists(path_principale + \"FILE_PATH/\" + path_validation)"]},{"cell_type":"markdown","metadata":{"id":"OU7A_9tjKHXD"},"source":["# **Configurazione mediapipe**"]},{"cell_type":"markdown","metadata":{"id":"SH0wRyX1I_uc"},"source":["## Installazione mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIFMaYcI1Yjh"},"outputs":[],"source":["!pip install mediapipe"]},{"cell_type":"markdown","metadata":{"id":"M9N_J_7OKJVL"},"source":["## Coordinate dei landmark delle labbra di mediapipe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pB4JnEDH39pv"},"outputs":[],"source":["FACEMESH_LIPS = [(61,61),(37,37),(0, 0)(267,267),(306,306),(314,314),(17, 17),(84,84)]"]},{"cell_type":"markdown","metadata":{"id":"8Hq0X9Y82NmQ"},"source":["# **Funzioni per il calcolo dei landmark**"]},{"cell_type":"markdown","metadata":{"id":"uxQBPpihQY9v"},"source":["## **Funzione che calcola la distanza tra due landmark sfruttando la distanza cityblock**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrIaes7G1u3z"},"outputs":[],"source":["from scipy.spatial import distance\n","\n","def calcola_distanza_landmark(selected_points):\n","  if any(math.isnan(val) for coord in selected_points for val in coord):\n","        return math.nan\n","  # Inizializzazione della variabile per la lunghezza totale\n","  total_length = 0\n","\n","  # Iterazione su tutti i landmark selezionati in ordine\n","  for i in range(len(selected_points)-1):\n","      # Estrazione delle coordinate del landmark corrente\n","      p1= selected_points[i]\n","      \n","      # Estrazione delle coordinate del landmark successivo\n","      p2 = selected_points[i+1]\n","      # cityblock, cosine, chebyshev, euclidean\n","      distanza = distance.cityblock(p1, p2)\n","      \n","      \n","      # Aggiornamento della lunghezza totale\n","      total_length += distanza\n","      #total_length = round(total_length,2)\n","  \n","  return total_length"]},{"cell_type":"markdown","metadata":{"id":"N4ItQLDYKxhZ"},"source":["## **Funzione che estrae la larghezza delle labbra in pixel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3bt9qW21zuf"},"outputs":[],"source":["# larghezza delle labbra su tre punti specifici\n","def estrazione_feature_larghezza(landmarks_points):\n","\n","  zona_superiore = [landmarks_points[37],  landmarks_points[267]]\n","\n","  zona_centrale =  [landmarks_points[61],  landmarks_points[306] ]\n","\n","  zona_inferiore = [landmarks_points[84],  landmarks_points[314]]\n","\n","  w1 = calcola_distanza_landmark(zona_superiore)\n","  w2 = calcola_distanza_landmark(zona_centrale)\n","  w3 = calcola_distanza_landmark(zona_inferiore)\n","\n","  return {\"37-267\":w1, \"61-306\": w2,\"84-314\": w3}"]},{"cell_type":"markdown","metadata":{"id":"S1g_xhwoK8no"},"source":["## **Funzione che estrae l'altezza delle labbra in pixel**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtcrKE6e17K3"},"outputs":[],"source":["# Altezza delle labbra utilizzando i tubercoli\n","def estrazione_feature_altezza(landmarks_points):\n","\n","  tubercolo_sinistro_superiore = [landmarks_points[37],  landmarks_points[84]]\n","\n","  tubercolo_centrale_superiore = [landmarks_points[0],  landmarks_points[17]]\n","\n","  tubercolo_destro_superiore = [landmarks_points[267],  landmarks_points[314]]\n","\n","  h1 = calcola_distanza_landmark(tubercolo_sinistro_superiore)\n","  h2 = calcola_distanza_landmark(tubercolo_centrale_superiore)\n","  h3 = calcola_distanza_landmark(tubercolo_destro_superiore)\n","\n","\n","  return {\"37-84\":h1,\"0-17\": h2,\"267-314\":h3}"]},{"cell_type":"markdown","metadata":{"id":"ZbbaQtajLDIn"},"source":["## **Funzione utilizzata per settare i valori dei landmark a (0,0) in caso che il volto non venga rilevato**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnACv3Va-lGB"},"outputs":[],"source":["# funzione che controlla se un landmark è stato trovato, se non è stato trovato setta il suo valore a NaN\n","def key_exists(landmarks_points):\n","  temp_value = [math.nan, math.nan]\n","  if 37 not in landmarks_points:\n","    landmarks_points[37] = temp_value\n","  if 267 not in landmarks_points:\n","    landmarks_points[267] = temp_value\n","  if 61 not in landmarks_points:\n","    landmarks_points[61] = temp_value\n","  if 306 not in landmarks_points:\n","    landmarks_points[306] = temp_value\n","  if 84 not in landmarks_points:\n","    landmarks_points[84] = temp_value\n","  if 314 not in landmarks_points:\n","    landmarks_points[314] = temp_value\n","  if 0 not in landmarks_points:\n","    landmarks_points[0] = temp_value\n","  if 17 not in landmarks_points:\n","    landmarks_points[17] = temp_value"]},{"cell_type":"markdown","metadata":{"id":"Gdyv13G7LMqD"},"source":["## **Funzione che genera un dizionario contenente le feature delle labbra**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCzUqdjKGzVP"},"outputs":[],"source":["def generaFeature(landmarks_points):\n","  key_exists(landmarks_points)\n","  feature2 = estrazione_feature_larghezza(landmarks_points)\n","  feature3 = estrazione_feature_altezza(landmarks_points)\n","\n","  return dict(feature2,  **feature3)"]},{"cell_type":"markdown","metadata":{"id":"7gRCp5XBqE4C"},"source":["# **Funzione per toglierre i NaN**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7cPHouJ6-Ia"},"outputs":[],"source":["import pandas as pd\n","\n","\n","# Funzione che riempie i valori NaN con la media mobile calcolata su una finestra mobile di lunghezza n_frame\n","# Il tutto a condizione che ci siano almeno 2 valori non NaN nella finestra (min_periods)\n","def fill_nan(row, n_frame):\n","    return row.fillna(row.rolling(min_periods=2, window=n_frame).mean())\n","\n","# Funzione per sostituire i valori NaN all'inizio del file\n","def remove_initial_nan(df):\n","  # Sostituzione dei valori NaN nelle colonne intermedie con la media dei successivi\n","  df_filled = df.copy()\n","  columns_to_fill = df.columns[1:-1]  # Seleziona tutte le colonne tranne la prima e l'ultima\n","\n","  for column in columns_to_fill:\n","      df_filled[column].fillna(df_filled[column].fillna(method='ffill').shift(-1).mean(), inplace=True)\n","    \n","  return df_filled\n","\n","# Funzione per rimuovere i valori NaN\n","def remove_nan(df,num_frames_per_second):\n","  df = remove_initial_nan(df)\n","  df = df.apply(fill_nan, axis=1, n_frame=num_frames_per_second) #Caso di NaN in altre posizioni\n","\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"h0DGakXtLYkw"},"source":["# **Generazione dei file in cui saranno contenute le path dei video**"]},{"cell_type":"markdown","metadata":{"id":"hFHustoO-yCQ"},"source":["## Creazione file path video CH-SIMS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0zXNzdmpBsT"},"outputs":[],"source":["import os\n","\n","# Cartella contenente la path del dataset\n","dataset_path = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Datasets/Dataset_1_CH-SIMS_CMU-MOSEI/CH-SIMS/Originali/Versione_sperimentazione_6_video_+_lunghi_per_soggetto-originali\"\n","\n","# Cartella in cui salvare i file\n","folder_save = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH\"\n","\n","\n","# Crea un file con il nome della directory in folder_save\n","file_path = os.path.join(folder_save, \"dataset_cinese.txt\")\n","\n","# Apre il file in modalità scrittura\n","with open(file_path, \"w\") as f:\n","  # Esplora ricorsivamente solo i file di livello più basso nella directory\n","  for root, dirs, files in os.walk(dataset_path, topdown=False):\n","    for file in files:\n","      file_path = os.path.join(root, file)\n","      f.write(file_path + \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"oPKmhPMEAI5H"},"source":["## Creazione file path video CMU-MOSEI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IqdtDV2UC5r"},"outputs":[],"source":["import os\n","\n","# Cartella contenente la path del dataset\n","dataset_path = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Datasets/Dataset_1_CH-SIMS_CMU-MOSEI/CMU-MOSEI/Originali/Versione_sperimentazione_6_video_+_lunghi_per_35_soggetti-originali\"\n","\n","# Cartella in cui salvare i file\n","folder_save = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH\"\n","\n","\n","# Crea un file con il nome della directory in folder_save\n","file_path = os.path.join(folder_save, \"dataset_inglese.txt\")\n","\n","# Apre il file in modalità scrittura\n","with open(file_path, \"w\") as f:\n","  # Esplora ricorsivamente solo i file di livello più basso nella directory\n","  for root, dirs, files in os.walk(dataset_path, topdown=False):\n","    for file in files:\n","      file_path = os.path.join(root, file)\n","      f.write(file_path + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"-4L_dtbuH9Dq"},"source":["# **Pre processing dataset CH-SIMS e CMU-MOSEI**"]},{"cell_type":"markdown","metadata":{"id":"RobsI7_LKygz"},"source":["## Lista contenente le path dei file dei video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i72v89y7KyhC"},"outputs":[],"source":["files_path = [\"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Testing_cinese.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Testing_inglese.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Training_cinese.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/Training_inglese.txt\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6y1Of-tuUUEZ"},"outputs":[],"source":["files_path = [\"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/dataset_cinese.txt\",\n","              \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/FILE_PATH/dataset_inglese.txt\"]"]},{"cell_type":"markdown","source":["## Estrazione feature dai Landmark"],"metadata":{"id":"a9NeGRKua6W1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXCpf7PFIg3O"},"outputs":[],"source":["import random\n","import cv2\n","import mediapipe as mp\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","fps = 24\n","num_frames = 240\n","num_frames_per_second = 6\n","num_seconds = num_frames // fps\n","\n","random_indices = []\n","for i in range(num_seconds):\n","    start_index = i * fps\n","    end_index = start_index + fps\n","    indices = list(range(start_index, end_index))\n","    random_indices.extend(random.sample(indices, num_frames_per_second))\n","\n","random_indices.sort()\n","\n","frames_to_process = [i for i in range(num_frames) if i in random_indices]\n","\n","# Caricamento del modello di rilevamento dei landmarks del volto\n","mp_drawing = mp.solutions.drawing_utils\n","mp_face_mesh = mp.solutions.face_mesh\n","\n","# Inizializza il rilevatore dei landmark del viso\n","face_mesh = mp_face_mesh.FaceMesh()\n","\n","\n","for path_file in files_path:\n","\n","  with open(path_file, mode='r') as file:\n","\n","    video_data = []  # Lista di tutti i dati dei video\n","    \n","    linee = file.readlines()\n","\n","    csv_filename = os.path.basename(path_file)\n","    csv_filename_without_extension = csv_filename.split(\".\")[0] #per la creazione del dataset voglio un nome senza .txt\n","    estensione = \"_cineseVsInglese.csv\"\n","\n","    csv_path = path_principale+\"CSV_DATASET_CH-SIMS_CMU-MOSEI/\"+ csv_filename_without_extension + estensione\n","    \n","    if \"cinese\" in csv_filename_without_extension: \n","        target = 0\n","    elif \"inglese\" in csv_filename_without_extension:\n","        target = 1 \n","    else: \n","        None\n","\n","    for video_path in tqdm(linee, desc=\"Elaborazione video \" + csv_filename_without_extension, unit=\"video\"):\n","        video_path = video_path.strip() # Rimuovi il carattere di a capo dalla stringa\n","\n","        # Apre il video in input\n","        cap = cv2.VideoCapture(str(video_path))\n","\n","        filename = os.path.basename(video_path) #Nome del file video\n","        filename_without_extension = filename.split(\".\")[0] #per la creazione del dataset voglio un nome senza .avi\n","       \n","        # Inizializza la lista dei dati del video\n","        video_data_video = []\n","\n","        # Loop attraverso tutti i frame del video\n","        frame_count = 0\n","        frame_data = {}\n","        #nome del frame\n","        frame_data['video-frame'] = filename_without_extension # Alla fine collego al nome del file il frame analizzato\n","        colonna = 0\n","\n","        while cap.isOpened() and frame_count < 240:\n","            \n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            if frame_count not in frames_to_process: #l'indice del frame non si trova in quelli random\n","              frame_count += 1\n","              continue\n","          \n","            # Converte l'immagine in RGB\n","            frame_RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            \n","            if frame_RGB is None:\n","              print(\"Frame errato, la funzione cv2.COLOR_BGR2RGB non è stata in grado di convertire l'immagine in RGB\")\n","              frame_count += 1\n","              continue\n","            else:\n","              results = face_mesh.process(frame_RGB)\n","\n","              if results is None:\n","                print(\"Qualcosa è andato storto!\")\n","                frame_count += 1\n","                continue\n","              else:\n","                # Memorizza i punti dei landmarks delle labbra\n","                landmarks_points = {}\n","                if results.multi_face_landmarks:\n","                    face_landmarks = results.multi_face_landmarks[0]\n","                    for connection in FACEMESH_LIPS:\n","                        landmark_1 = face_landmarks.landmark[connection[0]]\n","                        n = connection[0]\n","\n","                        # Salva i dati del landmark\n","                        landmarks_points[n] = [landmark_1.x, landmark_1.y]\n","\n","                \n","                feature = generaFeature(landmarks_points)\n","\n","                video_data_video\n","                \n","                # Crea il dizionario con i dati del frame\n","                frame_data['37-267_'+str(colonna)] = feature['37-267']\n","                frame_data['61-306_'+str(colonna)]= feature['61-306']\n","                frame_data['84-314_'+str(colonna)]= feature['84-314']\n","                frame_data['37-84_'+str(colonna)]= feature['37-84']\n","                frame_data['0-17_'+str(colonna)]= feature['0-17']\n","                frame_data['267-314_'+str(colonna)]= feature['267-314']\n","\n","                frame_count += 1\n","                colonna += 1 \n","        \n","        # Aggiungi il dizionario alla lista dei dati del video\n","        video_data_video.append(frame_data)\n","\n","        # Aggiungi i dati del video alla lista di tutti i dati\n","        video_data.extend(video_data_video)\n","\n","        # Chiudi il video\n","        cap.release()\n","\n","    # Crea il dataframe dai dati di tutti i video e visualizzalo\n","    df = pd.DataFrame(video_data)\n","\n","    # Check per valori NaN\n","    df = remove_nan(df,(num_frames_per_second*10))\n","\n","    # Aggiungi una colonna chiamata \"target\" al DataFrame\n","    df['target'] = target\n","    # Visualizza il DataFrame\n","    display(df)\n","\n","    print(100*\"-\")\n","    print(\"Generazione del file csv \", csv_filename_without_extension, estensione)\n","    print(100*\"-\")\n","    df.to_csv(csv_path, index=False)\n","\n","    file.close()\n","\n","# Chiudi il rilevatore dei landmark del viso\n","face_mesh.close()"]},{"cell_type":"markdown","metadata":{"id":"H2IshoKZGDYT"},"source":["## Concatenazione dei due dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCZr61z5GSuh"},"outputs":[],"source":["import pandas as pd\n","\n","path_dataset_cinese = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/dataset_cinese_cineseVsInglese.csv'\n","path_dataset_inglese = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/dataset_inglese_cineseVsInglese.csv'\n","\n","# Carica il DataFrame dal file\n","df1 = pd.read_csv(path_dataset_cinese)\n","df2 = pd.read_csv(path_dataset_inglese)\n","\n","result = pd.concat([df1, df2])\n","\n","dataset_name = \"Train_Test_cineseVsInglese.csv\"\n","path_concat_dataset = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/\"+ dataset_name\n","\n","print(100*\"-\")\n","print(\"Generazione del file csv \", dataset_name)\n","print(100*\"-\")\n","\n","result.to_csv(path_concat_dataset, index=False)\n","display(result)\n","print(\"Dataset \", dataset_name, \" creato\")\n","print(100*\"-\")"]},{"cell_type":"markdown","source":["Concatenazione dataset cinese e inglese solo labbra"],"metadata":{"id":"BiPDUhx19Yko"}},{"cell_type":"code","source":["import pandas as pd\n","\n","path_dataset_cinese_train = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/Training_cinese_cineseVsInglese_optical_flow_feature_S.csv'\n","path_dataset_inglese_train = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/Training_inglese_cineseVsInglese_optical_flow_feature_S.csv'\n","\n","path_dataset_cinese_test = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/Testing_cinese_cineseVsInglese_optical_flow_feature_S.csv'\n","path_dataset_inglese_test = '/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/Testing_inglese_cineseVsInglese_optical_flow_feature_S.csv'\n","\n","\n","# Carica il DataFrame dal file\n","df1 = pd.read_csv(path_dataset_cinese_train)\n","df2 = pd.read_csv(path_dataset_inglese_train)\n","\n","\n","# Carica il DataFrame dal file\n","df3 = pd.read_csv(path_dataset_cinese_test)\n","df4 = pd.read_csv(path_dataset_inglese_test)\n","\n","result = pd.concat([df1, df2])\n","\n","result2 = pd.concat([df3, df4])\n","\n","dataset_name = \"Train_cineseVsInglese.csv\"\n","path_concat_dataset = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/\"+ dataset_name\n","\n","print(100*\"-\")\n","print(\"Generazione del file csv \", dataset_name)\n","print(100*\"-\")\n","\n","result.to_csv(path_concat_dataset, index=False)\n","display(result)\n","print(\"Dataset \", dataset_name, \" creato\")\n","print(100*\"-\")\n","\n","\n","\n","dataset_name = \"Test_cineseVsInglese.csv\"\n","path_concat_dataset = \"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/\"+ dataset_name\n","result2.to_csv(path_concat_dataset, index=False)"],"metadata":{"id":"LFr7O3xl9kur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QtZ-Nw8zUE5d"},"source":["## **Generazione dataset di train e test**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2TREeQdRuK4"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","dataset_absolute_path =\"/content/drive/Shareddrives/Progetti FVAB 22 23 - VSR & BABELE/Gruppi/Gruppo 22/Progetto BABELE/CSV_DATASET_CH-SIMS_CMU-MOSEI/\"\n","dataset_name = \"Train_Test_cineseVsInglese.csv\"\n","\n","train_dataset = \"Train/Train_CH-SIMS_CMU-MOSEI_cineseVsInglese.csv\"\n","test_dataset = \"Test/Test_CH-SIMS_CMU-MOSEI_cineseVsInglese.csv\"\n","\n","# Carica il DataFrame\n","df = pd.read_csv(dataset_absolute_path+dataset_name)  # Sostituisci con il percorso e il nome del tuo file\n","\n","# Seleziona le colonne indipendenti (X) e la colonna target (y)\n","X = df.iloc[:, :-1]  # Seleziona tutte le colonne tranne l'ultima (target)\n","y = df['target']\n","\n","# Suddivisione del DataFrame in set di addestramento e test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Creazione dei nuovi DataFrame per addestramento e test\n","df_train = pd.concat([X_train, y_train], axis=1)\n","df_test = pd.concat([X_test, y_test], axis=1)\n","\n","# Salva i nuovi DataFrame in file separati\n","df_train.to_csv(dataset_absolute_path+train_dataset, index=False)\n","df_test.to_csv(dataset_absolute_path+test_dataset, index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}